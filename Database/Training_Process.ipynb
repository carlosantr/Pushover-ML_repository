{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08abacd0",
   "metadata": {},
   "source": [
    "# Practical Examples - Training\n",
    "This notebook present the trainig functions used for different ML techniques based on the hyperparameters of each model.\n",
    "* Note 1: Before training you have to load your dataset and define your X and Y variables.\n",
    "* Note 2: The Scalers are pre-processing tools used for normalization and standarization of the data (e.g. RobustScaler or MinMaxScaler from sklearn).\n",
    "* Note 3: Each problem is different, so you probably have to explore your data and define your pre-processing strategies and modify the training functions for each ML technique.\n",
    "* Note 4: We recommend exploring the hyperparameters of each technique and consult each model with the Tensorflow/Keras and Sklean documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ddc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import some libraries and frameworks\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb3f04",
   "metadata": {},
   "source": [
    "### Artificial Neural Networks (ANN)\n",
    "The function have as input:\n",
    "* X_train: Input variables for the training of the model (pandas Dataframe).\n",
    "* y_train: Output variable for the training of the model (pandas Series or Dataframe).\n",
    "* scalerX and scalerY: Scalers for X and Y data.\n",
    "* distribution: List of hidden layers and number of neurons in each one (e.g. [16,32,16] is 3 hidden layers with 16, 32, and 16 neurons, respectively).\n",
    "* activation: Activation function for the ANN (must be 'relu', 'leaky_relu' or 'elu').\n",
    "* regularizer: Type of regularization in each layers of the ANN (must be 'l1', 'l2' or None).\n",
    "* regularizer_value: Applied regularization value (usually 0.01, 0.001 or 0.0001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4799620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_ANN_Model(X_train, y_train, scalerX, scalerY,\n",
    "                    distribution, activation, regularizer, regularizer_value):\n",
    "    #Transformation of the data\n",
    "    X_train_scaled = pd.DataFrame(scalerX.transform(X_train), columns=X_train.columns)\n",
    "    y_train_scaled = scalerY.transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "    #Applying regularization\n",
    "    if regularizer == \"l1\":\n",
    "        regularizer_ = tf.keras.regularizers.l1(regularizer_value)\n",
    "    elif regularizer == \"l2\":\n",
    "        regularizer_ = tf.keras.regularizers.l2(regularizer_value)\n",
    "    else:\n",
    "        regularizer = None\n",
    "    #Adding the hidden layers of the ANN\n",
    "    red = []\n",
    "    n_capa = 0\n",
    "    for i in distribution:\n",
    "        n_capa += 1\n",
    "        if n_capa == 1:\n",
    "            red.append(tf.keras.layers.Dense(i, input_shape=(len(X_train.columns),), kernel_regularizer=regularizer_))\n",
    "        else:\n",
    "            red.append(tf.keras.layers.Dense(i, kernel_regularizer=regularizer_))\n",
    "        #Activation functions\n",
    "        if activation == 'relu':\n",
    "            red.append(tf.keras.layers.ReLU())\n",
    "        elif activation == 'leaky_relu':\n",
    "            red.append(tf.keras.layers.LeakyReLU(alpha=0.1))\n",
    "        elif activation == 'elu':\n",
    "            red.append(tf.keras.layers.ELU(alpha=1.0))\n",
    "    #Applying Batch Normalization and the output layer\n",
    "    red.append(tf.keras.layers.BatchNormalization())\n",
    "    red.append(tf.keras.layers.Dense(1, activation='linear'))\n",
    "    #Defining the model\n",
    "    regressor = tf.keras.Sequential(red)\n",
    "    #Compile the model\n",
    "    regressor.compile(optimizer='adam', loss='mse', metrics=[\"mae\"])\n",
    "    #Split the data into training and validation\n",
    "    X_t, X_v, Y_t, Y_v = train_test_split(X_train_scaled, y_train_scaled, test_size=0.3)\n",
    "    #Training the data for 100 epochs\n",
    "    regressor.fit(X_t, Y_t, epochs=100, validation_data=(X_v, Y_v), batch_size=32, verbose=0)\n",
    "    #Return the trained model\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80983910",
   "metadata": {},
   "source": [
    "### Random Forests (RF)\n",
    "The function have as input:\n",
    "* X_train: Input variables for the training of the model (pandas Dataframe).\n",
    "* y_train: Output variable for the training of the model (pandas Series or Dataframe).\n",
    "* scalerX and scalerY: Scalers for X and Y data.\n",
    "* n_estimator: Number of decision trees in RF model.\n",
    "* max_depth:Maximum depth of the trees.\n",
    "* min_samples_split: Minimum number of samples to split a node.\n",
    "* min_samples_leaf: Minimum number of samples to be at a leaf node.\n",
    "* max_feature: Number of features to consider at every split.\n",
    "* bootstrap_: Method of selecting samples for training trees (must be True or False).\n",
    "\n",
    "- Note 1: We highly recommend exploring how those values affect the performance of the model, avoiding problems such as overfitting or underfitting.\n",
    "- Note 2: Adjust the parameters n_jobs according to your PC capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db337f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_RF_Model(X_train, y_train, scalerX, scalerY,\n",
    "                   n_estimator, max_depth, min_samples_split, min_samples_leaf, max_feature , bootstrap_):\n",
    "    #Transformation of the data\n",
    "    X_train_scaled = pd.DataFrame(scalerX.transform(X_train), columns=X_train.columns)\n",
    "    y_train_scaled = scalerY.transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "    #Model definition\n",
    "    regressor = RandomForestRegressor(n_estimators=n_estimator,\n",
    "                                      max_depth=max_depth,\n",
    "                                      min_samples_split=min_samples_split,\n",
    "                                      min_samples_leaf=min_samples_leaf,\n",
    "                                      max_features=max_feature,\n",
    "                                      bootstrap=bootstrap_,\n",
    "                                      n_jobs=-1)\n",
    "    #Training\n",
    "    regressor.fit(X_train_scaled, y_train_scaled)\n",
    "    #Return the trained model\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988d815",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machines (GBM)\n",
    "The function have as input:\n",
    "* X_train: Input variables for the training of the model (pandas Dataframe).\n",
    "* y_train: Output variable for the training of the model (pandas Series or Dataframe).\n",
    "* scalerX and scalerY: Scalers for X and Y data.\n",
    "* n_estimator: Number of decision trees in RF model.\n",
    "* max_depth: Maximum depth of the trees.\n",
    "* max_feature: Number of features to consider at every split.\n",
    "* subsample: Fraction of samples in each tree (between 0 and 1)\n",
    "* learning_rate: Contribution of each tree (usually 0.01, 0.05 or 0.1).\n",
    "\n",
    "- Note 1: We highly recommend exploring how those values affect the performance of the model, avoiding problems such as overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_GBM_Model(X_train, y_train, scalerX, scalerY,\n",
    "                    n_estimator, max_depth, max_feature, subsample, learning_rate):\n",
    "    #Transformation of the data\n",
    "    X_train_scaled = pd.DataFrame(scalerX.transform(X_train), columns=X_train.columns)\n",
    "    y_train_scaled = scalerY.transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "    #Model definition\n",
    "    regressor = GradientBoostingRegressor(n_estimators=n_estimator,\n",
    "                                          learning_rate=learning_rate,\n",
    "                                          max_depth=max_depth,\n",
    "                                          max_features=max_feature,\n",
    "                                          subsample=subsample)\n",
    "    #Training\n",
    "    regressor.fit(X_train_scaled, y_train_scaled)\n",
    "    #Return the trained model\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc3677",
   "metadata": {},
   "source": [
    "### LASSO Regression (LASSO)\n",
    "The function have as input:\n",
    "* X_train: Input variables for the training of the model (pandas Dataframe).\n",
    "* y_train: Output variable for the training of the model (pandas Series or Dataframe).\n",
    "* scalerX and scalerY: Scalers for X and Y data.\n",
    "* alpha: Applied regularization (L1) (usually <0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_LASSO_Model(X_train, y_train, scalerX, scalerY,\n",
    "                      alpha):\n",
    "    #Transformation of the data\n",
    "    X_train_scaled = pd.DataFrame(scalerX.transform(X_train), columns=X_train.columns)\n",
    "    y_train_scaled = scalerY.transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "    #Model definition\n",
    "    regressor = Lasso(alpha=alpha)\n",
    "    #Training\n",
    "    regressor.fit(X_train_scaled, y_train_scaled)\n",
    "    #Return the trained model\n",
    "    return regressor"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
